!pip install pandas numpy torch transformers scikit-learn nltk imbalanced-learn vaderSentiment

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
import re
import gc
import warnings
import nltk
from transformers import DistilBertTokenizer, DistilBertModel, get_scheduler
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from torch.utils.data import Dataset, DataLoader
from nltk.sentiment import SentimentIntensityAnalyzer
from imblearn.over_sampling import RandomOverSampler
np.random.seed(40)
random.seed(40)
# Suppress Warnings
warnings.filterwarnings("ignore")

# Detect Device (CPU/GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load Dataset
df = pd.read_csv('fb_news_posts_20K.csv', quoting=3, on_bad_lines='skip')
df = df[['message', 'react_angry', 'react_haha', 'react_like', 'react_love', 'react_sad', 'react_wow']].dropna()

# Sentiment Classification (Using VADER for better labeling)
nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()

def classify_sentiment(text):
    score = sia.polarity_scores(text)['compound']
    if score <= -0.2:
        return 0  # Negative
    elif score >= 0.2:
        return 1  # Positive
    else:
        return 2  # Neutral

df['sentiment'] = df['message'].apply(classify_sentiment)

# Handle Class Imbalance with Oversampling
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(df[['message']], df['sentiment'])
df = pd.DataFrame({'message': X_resampled['message'], 'sentiment': y_resampled})

# Compute Class Weights
unique_classes = np.unique(df['sentiment'])
class_weights = compute_class_weight(class_weight='balanced', classes=unique_classes, y=df['sentiment'])
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
# Load DistilBERT Tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

# Define Custom Dataset
class FacebookDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = tokenizer(
            self.texts[idx],
            padding="max_length",
            truncation=True,
            max_length=256,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }

# Train-Test Split
train_texts, test_texts, train_labels, test_labels = train_test_split(df['message'].tolist(), df['sentiment'].tolist(), test_size=0.2, random_state=42)

train_dataset = FacebookDataset(train_texts, train_labels)
test_dataset = FacebookDataset(test_texts, test_labels)

# Optimized DataLoader
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)




# Define DistilBERT Classifier
class DistilBertClassifier(nn.Module):
    def __init__(self):
        super(DistilBertClassifier, self).__init__()
        self.bert = DistilBertModel.from_pretrained("distilbert-base-uncased")
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(768, 3)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = self.dropout(outputs.last_hidden_state[:, 0, :])
        logits = self.fc(pooled_output)
        return logits

# Define Label Smoothing Loss
class LabelSmoothingLoss(nn.Module):
    def __init__(self, smoothing=0.2):  # Increased smoothing
        super(LabelSmoothingLoss, self).__init__()
        self.smoothing = smoothing

    def forward(self, logits, labels):
        num_classes = logits.size(-1)
        confidence = 1.0 - self.smoothing
        label_one_hot = torch.zeros_like(logits).scatter_(1, labels.unsqueeze(1), confidence)
        label_one_hot += self.smoothing / num_classes
        return torch.mean(-label_one_hot * torch.log_softmax(logits, dim=-1))

# Initialize Model & Optimizer
model = DistilBertClassifier().to(device)
optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Increased LR for first 10 epochs
criterion = LabelSmoothingLoss()
scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 20)
# Training Loop
for epoch in range(3):
    model.train()
    total_loss, correct, total = 0, 0, 0

    for batch in train_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask)

        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, predicted = torch.max(outputs, dim=1)
        correct += (predicted == labels).sum().item()
        total += labels.size(0)
    print(f'Epoch {epoch+1}: Train Accuracy = {correct / total * 100:.2f}%')
# ✅ Final Test Accuracy
model.eval()
correct, total = 0, 0
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        _, predicted = torch.max(outputs, dim=1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

test_accuracy = correct / total * 100
print(f"✅ Final Test Accuracy: {test_accuracy:.2f}%")
